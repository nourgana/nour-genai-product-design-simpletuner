2025-03-26 13:49:08,078 [INFO] (SimpleTuner) Using json configuration backend.
2025-03-26 13:49:08,078 [INFO] (SimpleTuner) [CONFIG.JSON] Loaded configuration from config/config.json
2025-03-26 13:49:08,078 [WARNING] (SimpleTuner) Skipping false argument: --disable_benchmark
2025-03-26 13:49:08,078 [WARNING] (SimpleTuner) Skipping false argument: --validation_torch_compile
2025-03-26 13:49:08,086 [WARNING] (ArgsParser) The VAE model madebyollin/sdxl-vae-fp16-fix is not compatible. Please use a compatible VAE to eliminate this warning. The baked-in VAE will be used, instead.
2025-03-26 13:49:08,086 [INFO] (ArgsParser) VAE Model: black-forest-labs/FLUX.1-dev
2025-03-26 13:49:08,086 [INFO] (ArgsParser) Default VAE Cache location: 
2025-03-26 13:49:08,086 [INFO] (ArgsParser) Text Cache location: cache
2025-03-26 13:49:08,086 [WARNING] (ArgsParser) Updating T5 XXL tokeniser max length to 512 for Flux.
2025-03-26 13:49:08,087 [INFO] (ArgsParser) Enabled NVIDIA TF32 for faster training on Ampere GPUs. Use --disable_tf32 if this causes any problems.
2025-03-26 13:49:08,398 [INFO] (SimpleTuner) Load VAE: black-forest-labs/FLUX.1-dev
2025-03-26 13:49:09,601 [INFO] (SimpleTuner) Loading VAE onto accelerator, converting from torch.float32 to torch.bfloat16
2025-03-26 13:49:10,011 [INFO] (SimpleTuner) Load tokenizers
2025-03-26 13:49:10,885 [INFO] (helpers.training.text_encoding) Loading OpenAI CLIP-L text encoder from black-forest-labs/FLUX.1-dev/text_encoder..
2025-03-26 13:49:11,295 [INFO] (helpers.training.text_encoding) Loading T5 XXL v1.1 text encoder from black-forest-labs/FLUX.1-dev/text_encoder_2..
2025-03-26 13:49:17,098 [INFO] (SimpleTuner) Moving text encoder to GPU.
2025-03-26 13:49:18,274 [INFO] (SimpleTuner) Moving text encoder 2 to GPU.
2025-03-26 13:50:12,168 [INFO] (DataBackendFactory) Loading data backend config from config/multidatabackend.json
2025-03-26 13:50:12,169 [INFO] (DataBackendFactory) Configuring text embed backend: text-embed-cache
2025-03-26 13:50:12,437 [INFO] (TextEmbeddingCache) (Rank: 0) (id=text-embed-cache) Listing all text embed cache entries
2025-03-26 13:50:12,457 [INFO] (DataBackendFactory) Pre-computing null embedding
2025-03-26 13:50:12,457 [WARNING] (DataBackendFactory) Not using caption dropout will potentially lead to overfitting on captions, eg. CFG will not work very well. Set --caption_dropout_probability=0.1 as a recommended value.
2025-03-26 13:50:12,458 [INFO] (DataBackendFactory) Completed loading text embed services.
2025-03-26 13:50:12,458 [INFO] (DataBackendFactory) Configuring data backend: classic-fusion-768
2025-03-26 13:50:12,459 [INFO] (DataBackendFactory) (id=classic-fusion-768) Loading bucket manager.
2025-03-26 13:50:12,462 [INFO] (DataBackendFactory) (id=classic-fusion-768) Refreshing aspect buckets on main process.
2025-03-26 13:50:12,462 [INFO] (BaseMetadataBackend) Discovering new files...
2025-03-26 13:50:12,508 [INFO] (BaseMetadataBackend) Compressed 875 existing files from 5.
2025-03-26 13:50:12,508 [INFO] (BaseMetadataBackend) No new files discovered. Doing nothing.
2025-03-26 13:50:12,508 [INFO] (BaseMetadataBackend) Statistics: {'total_processed': 0, 'skipped': {'already_exists': 875, 'metadata_missing': 0, 'not_found': 0, 'too_small': 0, 'other': 0}}
2025-03-26 13:50:12,515 [WARNING] (DataBackendFactory) Key crop_aspect not found in the current backend config, using the existing value 'square'.
2025-03-26 13:50:12,515 [WARNING] (DataBackendFactory) Key crop_style not found in the current backend config, using the existing value 'random'.
2025-03-26 13:50:12,515 [WARNING] (DataBackendFactory) Key disable_validation not found in the current backend config, using the existing value 'False'.
2025-03-26 13:50:12,515 [WARNING] (DataBackendFactory) Key config_version not found in the current backend config, using the existing value '1'.
2025-03-26 13:50:12,515 [WARNING] (DataBackendFactory) Key hash_filenames not found in the current backend config, using the existing value 'True'.
2025-03-26 13:50:12,515 [INFO] (DataBackendFactory) Configured backend: {'id': 'classic-fusion-768', 'config': {'repeats': 10, 'crop': False, 'crop_aspect': 'square', 'crop_style': 'random', 'disable_validation': False, 'resolution': 0.589824, 'resolution_type': 'area', 'caption_strategy': 'textfile', 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'maximum_image_size': None, 'target_downsample_size': None, 'config_version': 1, 'hash_filenames': True}, 'dataset_type': 'image', 'data_backend': <helpers.data_backend.local.LocalDataBackend object at 0x7f79bd1eb730>, 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'metadata_backend': <helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x7f79bd1eb6d0>}
2025-03-26 13:50:12,516 [INFO] (DataBackendFactory) (id=classic-fusion-768) Collecting captions.
2025-03-26 13:50:12,877 [INFO] (DataBackendFactory) (id=classic-fusion-768) Initialise text embed pre-computation using the textfile caption strategy. We have 875 captions to process.
2025-03-26 13:50:12,899 [INFO] (DataBackendFactory) (id=classic-fusion-768) Completed processing 875 captions.
2025-03-26 13:50:12,899 [INFO] (DataBackendFactory) (id=classic-fusion-768) Creating VAE latent cache.
2025-03-26 13:50:12,904 [INFO] (DataBackendFactory) (id=classic-fusion-768) Discovering cache objects..
2025-03-26 13:50:12,940 [INFO] (DataBackendFactory) Configured backend: {'id': 'classic-fusion-768', 'config': {'repeats': 10, 'crop': False, 'crop_aspect': 'square', 'crop_style': 'random', 'disable_validation': False, 'resolution': 0.589824, 'resolution_type': 'area', 'caption_strategy': 'textfile', 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'maximum_image_size': None, 'target_downsample_size': None, 'config_version': 1, 'hash_filenames': True}, 'dataset_type': 'image', 'data_backend': <helpers.data_backend.local.LocalDataBackend object at 0x7f79bd1eb730>, 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'metadata_backend': <helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x7f79bd1eb6d0>, 'train_dataset': <helpers.multiaspect.dataset.MultiAspectDataset object at 0x7f79bd0e6230>, 'sampler': <helpers.multiaspect.sampler.MultiAspectSampler object at 0x7f79bd0e6b00>, 'train_dataloader': <torch.utils.data.dataloader.DataLoader object at 0x7f79bd0e60e0>, 'text_embed_cache': <helpers.caching.text_embeds.TextEmbeddingCache object at 0x7f79bad6a170>, 'vaecache': <helpers.caching.vae.VAECache object at 0x7f79badc1a50>}
2025-03-26 13:50:12,946 [INFO] (DataBackendFactory) Configuring data backend: classic-fusion-crop-768
2025-03-26 13:50:12,947 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Loading bucket manager.
2025-03-26 13:50:12,948 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Refreshing aspect buckets on main process.
2025-03-26 13:50:12,948 [INFO] (BaseMetadataBackend) Discovering new files...
2025-03-26 13:50:12,993 [INFO] (BaseMetadataBackend) Compressed 875 existing files from 1.
2025-03-26 13:50:12,993 [INFO] (BaseMetadataBackend) No new files discovered. Doing nothing.
2025-03-26 13:50:12,994 [INFO] (BaseMetadataBackend) Statistics: {'total_processed': 0, 'skipped': {'already_exists': 875, 'metadata_missing': 0, 'not_found': 0, 'too_small': 0, 'other': 0}}
2025-03-26 13:50:12,999 [WARNING] (DataBackendFactory) Key crop_aspect_buckets not found in the current backend config, using the existing value 'None'.
2025-03-26 13:50:12,999 [WARNING] (DataBackendFactory) Key disable_validation not found in the current backend config, using the existing value 'False'.
2025-03-26 13:50:12,999 [WARNING] (DataBackendFactory) Key config_version not found in the current backend config, using the existing value '2'.
2025-03-26 13:50:12,999 [WARNING] (DataBackendFactory) Key hash_filenames not found in the current backend config, using the existing value 'True'.
2025-03-26 13:50:12,999 [INFO] (DataBackendFactory) Configured backend: {'id': 'classic-fusion-crop-768', 'config': {'vae_cache_clear_each_epoch': False, 'repeats': 10, 'crop': True, 'crop_aspect': 'square', 'crop_aspect_buckets': None, 'crop_style': 'center', 'disable_validation': False, 'resolution': 0.589824, 'resolution_type': 'area', 'caption_strategy': 'textfile', 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'maximum_image_size': None, 'target_downsample_size': None, 'config_version': 2, 'hash_filenames': True}, 'dataset_type': 'image', 'data_backend': <helpers.data_backend.local.LocalDataBackend object at 0x7f79a0aedba0>, 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'metadata_backend': <helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x7f79a0aed9f0>}
2025-03-26 13:50:13,000 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Collecting captions.
2025-03-26 13:50:13,014 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Initialise text embed pre-computation using the textfile caption strategy. We have 875 captions to process.
2025-03-26 13:50:13,057 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Completed processing 875 captions.
2025-03-26 13:50:13,057 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Creating VAE latent cache.
2025-03-26 13:50:13,062 [INFO] (DataBackendFactory) (id=classic-fusion-crop-768) Discovering cache objects..
2025-03-26 13:50:13,099 [INFO] (DataBackendFactory) Configured backend: {'id': 'classic-fusion-crop-768', 'config': {'vae_cache_clear_each_epoch': False, 'repeats': 10, 'crop': True, 'crop_aspect': 'square', 'crop_aspect_buckets': None, 'crop_style': 'center', 'disable_validation': False, 'resolution': 0.589824, 'resolution_type': 'area', 'caption_strategy': 'textfile', 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'maximum_image_size': None, 'target_downsample_size': None, 'config_version': 2, 'hash_filenames': True}, 'dataset_type': 'image', 'data_backend': <helpers.data_backend.local.LocalDataBackend object at 0x7f79a0aedba0>, 'instance_data_dir': '/home/GANA/nour-genai-product-design-simpletuner/datasets/classic_fusion', 'metadata_backend': <helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x7f79a0aed9f0>, 'train_dataset': <helpers.multiaspect.dataset.MultiAspectDataset object at 0x7f79a1a017e0>, 'sampler': <helpers.multiaspect.sampler.MultiAspectSampler object at 0x7f79a1a01630>, 'train_dataloader': <torch.utils.data.dataloader.DataLoader object at 0x7f79a1a03400>, 'text_embed_cache': <helpers.caching.text_embeds.TextEmbeddingCache object at 0x7f79bad6a170>, 'vaecache': <helpers.caching.vae.VAECache object at 0x7f79a1a03070>}
2025-03-26 13:50:13,955 [INFO] (validation) Precomputing the negative prompt embed for validations.
2025-03-26 13:50:14,134 [INFO] (SimpleTuner) Calculated our maximum training steps at 10000 because we have 1 epochs and 19250 steps per epoch.
2025-03-26 13:50:14,134 [INFO] (SimpleTuner) Collected the following data backends: ['text-embed-cache', 'classic-fusion-768', 'classic-fusion-crop-768']
2025-03-26 13:50:14,134 [INFO] (SimpleTuner) Unloading text encoders, as they are not being trained.
2025-03-26 13:50:18,544 [INFO] (SimpleTuner) After nuking text encoders from orbit, we freed 9.11 GB of VRAM. The real memories were the friends we trained a model on along the way.
2025-03-26 13:50:18,789 [INFO] (SimpleTuner) After nuking the VAE from orbit, we freed 163.84 MB of VRAM.
2025-03-26 13:50:20,953 [INFO] (SimpleTuner) Moving transformer to dtype=torch.bfloat16, device=cuda
2025-03-26 13:52:35,401 [INFO] (helpers.training.quantisation) Quantising FluxTransformer2DModelWithMasking. Using int8-quanto.
2025-03-26 13:52:35,401 [INFO] (helpers.training.quantisation) Freezing model weights only
2025-03-26 13:52:36,094 [INFO] (SimpleTuner) Using LoRA training mode (rank=16)
2025-03-26 13:52:36,900 [INFO] (SimpleTuner) Moving transformer to dtype=torch.bfloat16, device=cuda
2025-03-26 13:52:37,013 [INFO] (SimpleTuner) Moving the diffusion transformer to GPU in int8-quanto precision.
2025-03-26 13:52:37,125 [INFO] (validation) Using model evaluator: <helpers.training.evaluation.CLIPModelEvaluator object at 0x7f79badc2f80>
2025-03-26 13:52:37,125 [INFO] (SimpleTuner) Learning rate: 0.0001
2025-03-26 13:52:37,126 [INFO] (helpers.training.optimizer_param) cls: <class 'helpers.training.optimizers.adamw_bfloat16.AdamWBF16'>, settings: {'betas': (0.9, 0.999), 'weight_decay': 0.01, 'eps': 1e-06}
2025-03-26 13:52:37,134 [INFO] (SimpleTuner) Optimizer arguments={'lr': 0.0001, 'betas': (0.9, 0.999), 'weight_decay': 0.01, 'eps': 1e-06}
2025-03-26 13:52:37,135 [INFO] (SimpleTuner) Loading polynomial learning rate scheduler with 100 warmup steps
2025-03-26 13:52:37,135 [INFO] (SimpleTuner) Using Polynomial learning rate scheduler with last epoch -2.
2025-03-26 13:52:37,140 [INFO] (SimpleTuner) Preparing models..
2025-03-26 13:52:37,141 [INFO] (SimpleTuner) Loading our accelerator...
2025-03-26 13:52:37,285 [INFO] (SimpleTuner) Resuming from checkpoint checkpoint-10000
2025-03-26 13:52:37,498 [INFO] (root) gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c /var/tmp/tmpajimz_mj/test.c -o /var/tmp/tmpajimz_mj/test.o
2025-03-26 13:52:37,648 [INFO] (root) gcc -pthread -B /opt/conda/compiler_compat /var/tmp/tmpajimz_mj/test.o -laio -o /var/tmp/tmpajimz_mj/a.out
2025-03-26 13:52:37,697 [INFO] (root) gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c /var/tmp/tmpnqp2hckc/test.c -o /var/tmp/tmpnqp2hckc/test.o
2025-03-26 13:52:37,716 [INFO] (root) gcc -pthread -B /opt/conda/compiler_compat /var/tmp/tmpnqp2hckc/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /var/tmp/tmpnqp2hckc/a.out
2025-03-26 13:52:39,784 [INFO] (MultiAspectSampler-classic-fusion-768) Previous checkpoint had 4 exhausted buckets.
2025-03-26 13:52:39,784 [INFO] (MultiAspectSampler-classic-fusion-768) Previous checkpoint was on epoch 6.
2025-03-26 13:52:39,784 [INFO] (MultiAspectSampler-classic-fusion-768) Previous checkpoint had 642 seen images.
2025-03-26 13:52:39,786 [INFO] (MultiAspectSampler-classic-fusion-crop-768) Previous checkpoint had 0 exhausted buckets.
2025-03-26 13:52:39,786 [INFO] (MultiAspectSampler-classic-fusion-crop-768) Previous checkpoint was on epoch 6.
2025-03-26 13:52:39,786 [INFO] (MultiAspectSampler-classic-fusion-crop-768) Previous checkpoint had 608 seen images.
2025-03-26 13:52:39,786 [INFO] (SimpleTuner) Resuming from global_step 10000.
2025-03-26 13:52:39,787 [INFO] (MultiAspectSampler-classic-fusion-768) 
(Rank: 0)     -> Number of seen images: 642
(Rank: 0)     -> Number of unseen images: 233
(Rank: 0)     -> Current Bucket: None
(Rank: 0)     -> 5 Buckets: ['0.56', '0.71', '0.67', '0.6', '0.79']
(Rank: 0)     -> 4 Exhausted Buckets: ['0.79', '0.6', '0.67', '0.56']
2025-03-26 13:52:39,787 [INFO] (MultiAspectSampler-classic-fusion-crop-768) 
(Rank: 0)     -> Number of seen images: 608
(Rank: 0)     -> Number of unseen images: 267
(Rank: 0)     -> Current Bucket: None
(Rank: 0)     -> 1 Buckets: ['1.0']
(Rank: 0)     -> 0 Exhausted Buckets: []
2025-03-26 13:52:39,803 [INFO] (SimpleTuner) 
***** Running training *****
-  Num batches = 19250
-  Num Epochs = 1
  - Current Epoch = 1
-  Total train batch size (w. parallel, distributed & accumulation) = 1
  - Instantaneous batch size per device = 1
  - Gradient Accumulation steps = 1
-  Total optimization steps = 10000
  - Steps completed: 10000
-  Total optimization steps remaining = 0
2025-03-26 13:52:41,775 [INFO] (SimpleTuner) Training has completed.
 -> global_step = 10001, max_train_steps = 10000, epoch = 1, num_train_epochs = 1
2025-03-26 13:52:41,775 [INFO] (SimpleTuner) Exiting training loop. Beginning model unwind at epoch 1, step 10001
2025-03-26 13:53:14,262 [INFO] (validation) Evaluation result: {'clip/min': 17.3543, 'clip/max': 30.1578, 'clip/mean': np.float64(23.756050000000002), 'clip/std': np.float64(6.401750000000002)}
